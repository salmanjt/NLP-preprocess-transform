{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Text Preprocessing & Feature Transformation](#toc0_)\n",
    "\n",
    "**Author Name:** Salman Tahir  \n",
    "**Environment:** Conda 23.7.2, Python 3.10.12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "1. [Text Preprocessing & Feature Transformation](#toc1_)    \n",
    "2. [Introduction](#toc2_)    \n",
    "3. [Importing Libraries](#toc3_)    \n",
    "4. [Reading Data](#toc4_)    \n",
    "5. [Downloading PDF Files](#toc5_)    \n",
    "6. [Aggregating Data from PDFs](#toc6_)    \n",
    "7. [Extracting Information to Entities](#toc7_)    \n",
    "8. [Preprocessing Data](#toc8_)    \n",
    "8.1. [Sentence Segmentation](#toc8_1_)    \n",
    "8.2. [Tokenization](#toc8_2_)    \n",
    "8.3. [Bigrams](#toc8_3_)    \n",
    "8.4. [Further Text Preprocessing](#toc8_4_)    \n",
    "8.5. [Stemming](#toc8_5_)    \n",
    "9. [Feature Conversion and Output](#toc9_)    \n",
    "10. [Statistical Summary](#toc10_)    \n",
    "11. [Summary](#toc11_)    \n",
    "12. [References](#toc12_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=true\n",
    "\tminLevel=1\n",
    "\tmaxLevel=2\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[Introduction](#toc0_)\n",
    "\n",
    "In this project, our primary goal is to bridge the gap between textual information and numerical representations, catering to the needs of advanced Natural Language Processing (NLP) systems and algorithms. Our focus lies in the preprocessing of a diverse dataset of published papers, transforming them into a format that is not only amenable to NLP applications but also highly suitable for downstream modeling tasks.\n",
    "\n",
    "Furthermore, we generate a statistical summary of the top 10 most frequent words in the titles, authors and abstracts of the papers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[Importing Libraries](#toc0_)\n",
    "\n",
    "The following libraries are used:\n",
    "\n",
    "-   `os` (for file path manipulation)\n",
    "-   `re` (for regular expressions and pattern matching)\n",
    "-   `csv` (for writing to csv files)\n",
    "-   `nltk` (for tokenization and stemming using PorterStemmer)\n",
    "-   `requests` (for downloading files from the http links)\n",
    "-   `concurrent.futures` (for multithreading utilising the ThreadPoolExecutor)\n",
    "-   `pdftotext` (for converting pdf tables to text)\n",
    "-   `pypdf` (for converting pdf files to text)\n",
    "-   `itertools` (for creating combinations of words)\n",
    "-   `google.oauth2` (for authenticating with google drive API)\n",
    "-   `collections` (for counting the frequency of words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import requests\n",
    "import pdftotext\n",
    "import itertools\n",
    "from pypdf import PdfReader\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.credentials import Credentials\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import Counter\n",
    "from nltk.collocations import *\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import MWETokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[Reading Data](#toc0_)\n",
    "\n",
    "We start by reading the initial PDF file and extracting the links to the papers in the PDF table.\n",
    "\n",
    "-   The extracted data is stored in a dictionary.\n",
    "-   The keys of the dictionary are the paper IDs.\n",
    "-   The values of the dictionary are the links to the papers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the PDF file and read its contents\n",
    "with open(\"../data/input/papers.pdf\", \"rb\") as file:\n",
    "    reader = PdfReader(file)\n",
    "    num_pages = len(reader.pages)\n",
    "    data = {}\n",
    "\n",
    "    # Extract URLs and filenames from table in the PDF file\n",
    "    for page in range(num_pages):\n",
    "        page_obj = reader.pages[page]\n",
    "        page_text = page_obj.extract_text()\n",
    "        lines = page_text.split(\"\\n\")\n",
    "        for line in lines:\n",
    "            if \"http\" in line:\n",
    "                # Extract filename from the line before URL\n",
    "                filename = lines[lines.index(line) - 1]\n",
    "                data[filename] = line\n",
    "\n",
    "    # Create a new dictionary with modified keys\n",
    "    articles = {key[:-4]: value for key, value in data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: PP3206\n",
      "URL: https://drive.google.com/uc?export=download&id=1KXR-_25SCUzukdgBlYVVskKd_sOcK1M0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print first item in the dictionary\n",
    "for key, value in list(articles.items())[:1]:\n",
    "    print(f\"Filename: {key}\\nURL: {value}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id='toc5_'></a>[Downloading PDF Files](#toc0_)\n",
    "\n",
    "-   We start by setting up our credentials for the google drive API.\n",
    "    -   Doing so allows us to bypass the rate limit enforced by google drive.\n",
    "-   Download the PDF files into a directory called `pdf_files`.\n",
    "    -   Note, that we also ensure to append the file extension to the file name.\n",
    "-   Using multithreading we download the files in parallel.\n",
    "\n",
    "**Before running the code block below, please ensure you have the `credentials.json` file available in the same directory as this notebook.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up credentials for Google Drive API\n",
    "# credentials = service_account.Credentials.from_service_account_file(\n",
    "#     'credentials.json')\n",
    "\n",
    "# # Create a directory to store PDF files\n",
    "# if not os.path.exists(\"../data/input/pdf_files/\"):\n",
    "#     os.makedirs(\"../data/input/pdf_files/\")\n",
    "\n",
    "\n",
    "# def download_pdf(url, filename):\n",
    "#     \"\"\"\n",
    "#     Downloads the PDF file from the given URL and saves it to the pdf_files directory.\n",
    "#     :param url: URL of the PDF file\n",
    "#     :param filename: Filename of the PDF file\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Use credentials to access the Google Drive API\n",
    "#         response = requests.get(\n",
    "#             url, headers={\"Authorization\": f\"Bearer {credentials.token}\"})\n",
    "#         with open(f\"pdf_files/{filename}.pdf\", \"wb\") as f:\n",
    "#             f.write(response.content)\n",
    "#         # Print filename of the downloaded PDF file to verify status\n",
    "#         print(f\"Downloaded {filename}.pdf\")\n",
    "#     except:\n",
    "#         # Print filename of the PDF file that failed to download\n",
    "#         print(f\"Error downloading {filename}.pdf from URL: {url}\")\n",
    "\n",
    "\n",
    "# # To download PDF files in parallel\n",
    "# with ThreadPoolExecutor() as executor:\n",
    "#     for filename, url in articles.items():\n",
    "#         executor.submit(download_pdf, url, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 200 files in the pdf_files directory.\n"
     ]
    }
   ],
   "source": [
    "# Count the number of files in directory\n",
    "num_files = len([f for f in os.listdir('../data/input/pdf_files/')\n",
    "                if os.path.isfile(os.path.join('../data/input/pdf_files/', f))])\n",
    "\n",
    "print(f\"There are {num_files} files in the {'pdf_files'} directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. <a id='toc6_'></a>[Aggregating Data from PDFs](#toc0_)\n",
    "\n",
    "Now, we extract all text from the PDF files into a dictionary.\n",
    "\n",
    "-   We remove the file extension from the file name and use it as the key for the dictionary.\n",
    "-   The values of the dictionary are the extracted text from the PDF files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to the directory containing the PDF files\n",
    "pdf_directory = \"../data/input/pdf_files/\"\n",
    "pdf_files = os.listdir(pdf_directory)\n",
    "\n",
    "# Create a dictionary to store text from PDF files\n",
    "text_dict = {}\n",
    "\n",
    "# Iterate through PDF files in the directory and extract text\n",
    "for file_name in pdf_files:\n",
    "    # Check to confirm that the file is a PDF file\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(pdf_directory, file_name)\n",
    "        with open(file_path, \"rb\") as pdf_file:\n",
    "            pdf = pdftotext.PDF(pdf_file)\n",
    "            text = \"\\n\".join(pdf)\n",
    "            # Remove file extension from the filename\n",
    "            key = file_name[:-4]\n",
    "            # Add text to our dictionary\n",
    "            text_dict[key] = text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. <a id='toc7_'></a>[Extracting Information to Entities](#toc0_)\n",
    "\n",
    "Once we have have our dictionary containing the extracted text from the PDF files, we can extract the required entities.\n",
    "\n",
    "We start by compiling the regular expressions for extracting the required entities.\n",
    "\n",
    "**Regular expression for extracting Titles**\n",
    "\n",
    "```python\n",
    "r'^(.+?)Authored'\n",
    "```\n",
    "\n",
    "-   `^(.+?)` matches all text until the word `Authored` is found.\n",
    "-   `Authored` matches the literal word `Authored`.\n",
    "\n",
    "**Regular expression for extracting Authors**\n",
    "\n",
    "```python\n",
    "r'(?<=Authored by:)(?:\\s*)([A-Za-z\\s.?-]+)(?=\\n\\s*Abstract)'\n",
    "```\n",
    "\n",
    "-   `(?<=Authored by:)` matches the literal word `Authored by:` and ensures that the match is not included in the result.\n",
    "-   `(?:\\s*)` matches any whitespace characters.\n",
    "-   `([A-Za-z\\s.?-]+)` matches all text until the next newline character is found.\n",
    "-   `(?=\\n\\s*Abstract)` matches the literal word `Abstract` and ensures that the match is not included in the result.\n",
    "\n",
    "**Regular expression for extracting Abstract**\n",
    "\n",
    "```python\n",
    "r'Abstract(.+?)\\s*1\\s*[\\n\\s]*Paper Body'\n",
    "```\n",
    "\n",
    "-   `Abstract` matches the literal word `Abstract`.\n",
    "-   `(.+?)` matches all text until the word `Paper Body` is found.\n",
    "-   `\\s*1\\s*` matches the literal word `1`.\n",
    "-   `[\\n\\s]*` matches any whitespace characters.\n",
    "-   `Paper Body` matches the literal word `Paper Body`.\n",
    "\n",
    "**Regular expression for extracting Paper Bodies**\n",
    "\n",
    "```python\n",
    "r'1\\s*Paper Body(.+?)2\\s*References'\n",
    "```\n",
    "\n",
    "-   `1\\s*Paper Body` matches the literal word `1 Paper Body`.\n",
    "-   `(.+?)` matches all text until the word `2 References` is found.\n",
    "-   `2\\s*References` matches the literal word `2 References`.\n",
    "\n",
    "Once, we have used the regular expressions to extract the required entities, we can perform some preprocessing on the data.\n",
    "\n",
    "-   We remove newlines and multiple spaces from the data.\n",
    "-   Note that before removing multiple spaces for authors, we perform a split using double space as the delimiter.\n",
    "-   Finally, the data is stored in a dictionary.\n",
    "    -   The keys of the dictionary are the paper IDs.\n",
    "    -   The values of the dictionary are the extracted entities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile regular expression for titles\n",
    "TITLE_PATTERN = re.compile(r'^(.+?)Authored', re.DOTALL)\n",
    "\n",
    "# Compile regular expression for authors\n",
    "AUTHOR_PATTERN = re.compile(\n",
    "    r'(?<=Authored by:)(?:\\s*)([A-Za-z\\s.?-]+)(?=\\n\\s*Abstract)', re.DOTALL)\n",
    "\n",
    "# Compile regular expression for abstracts\n",
    "ABSTRACT_PATTERN = re.compile(\n",
    "    r'Abstract(.+?)\\s*1\\s*[\\n\\s]*Paper Body', re.DOTALL)\n",
    "\n",
    "# Compile regular expression for paper bodies\n",
    "PAPER_PATTERN = re.compile(\n",
    "    r'1\\s*Paper Body(.+?)2\\s*References', re.DOTALL)\n",
    "\n",
    "\n",
    "# Create dictionaries to store extracted data\n",
    "titles = {}\n",
    "authors = {}\n",
    "abstracts = {}\n",
    "papers = {}\n",
    "\n",
    "\n",
    "for file_name, text in text_dict.items():\n",
    "    # Extract title\n",
    "    title = TITLE_PATTERN.findall(text)\n",
    "    # Remove extra spaces from title\n",
    "    title = re.sub(r'\\s+', ' ', title[0].strip())\n",
    "    # Add title to dictionary\n",
    "    titles[file_name] = title\n",
    "\n",
    "    # Extract author\n",
    "    author = AUTHOR_PATTERN.findall(text)\n",
    "    authors_list = []\n",
    "    for a in author:\n",
    "        # Remove newlines and extra spaces from author name\n",
    "        clean_author = re.sub(r'[\\n\\r]+', ' ', a.strip())\n",
    "        # Split multiple authors using delimiter \"  \"\n",
    "        authors_list.extend(clean_author.split(\"  \"))\n",
    "        # Remove empty strings from list\n",
    "        authors_list = list(filter(None, authors_list))\n",
    "        # Add author to dictionary\n",
    "        authors[file_name] = authors_list\n",
    "\n",
    "    # Extract abstract\n",
    "    abstract = ABSTRACT_PATTERN.findall(text)\n",
    "    # Remove newlines and extra spaces from abstract\n",
    "    clean_abstract = re.sub(r'\\s+', ' ', abstract[0].strip())\n",
    "    clean_abstract = re.sub(r'\\r\\n', ' ', clean_abstract)\n",
    "    # Add the abstract to dictionary\n",
    "    abstracts[file_name] = clean_abstract\n",
    "\n",
    "    # Extract the paper\n",
    "    paper = PAPER_PATTERN.findall(text)\n",
    "    # Remove newlines and extra spaces from the paper\n",
    "    clean_paper = re.sub(r'\\s+', ' ', paper[0].strip())\n",
    "    # Add the paper to our dictionary\n",
    "    papers[file_name] = clean_paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: PP3206\n",
      "Title: Learning the 2-D Topology of Images\n",
      "\n",
      "Filename: PP3206\n",
      "Authors: ['Yoshua Bengio', 'Bal?zs K?gl', 'Nicolas L. Roux', 'Pascal Lamblin', ' Marc Joliveau']\n",
      "\n",
      "Filename: PP3206\n",
      "Abstract: We study the following question: is the two-dimensional structure of images a very strong prior or is it something that can be learned with a few examples of natural images? If someone gave us a learning task involving images for which the two-dimensional topology of pixels was not known, could we discover it automatically and exploit it? For example suppose that the pixels had been permuted in a fixed but unknown way, could we recover the relative two-dimensional location of pixels on images? The surprising result presented here is that not only the answer is yes but that about as few as a thousand images are enough to approximately recover the relative locations of about a thousand pixels. This is achieved using a manifold learning algorithm applied to pixels associated with a measure of distributional similarity between pixel intensities. We compare different topology-extraction approaches and show how having the two-dimensional topology can be exploited.\n",
      "\n",
      "Filename: PP3206\n",
      "PaperBody: Machine learning has been applied to a number of tasks involving an input domain with a special topology: one-dimensional for sequences, two-dimensional for images, three-dimensional for videos and for 3-D capture. Some learning algorithms are generic, e.g., working on arbitrary unstructured vectors in d , such as ordinary SVMs, decision trees, neural networks, and boosting applied to generic learning algorithms. On the other hand, other learning algorithms successfully exploit the specific topology of their input, e.g., SIFT-based machine vision [10], convolutional neural networks [6, 7], time-delay neural networks [5, 16]. It has been conjectured [8, 2] that the two-dimensional structure of natural images is a very strong prior that would require a huge number of bits to specify, if starting from the completely uniform prior over all possible permutations. The question studied here is the following: is the two-dimensional structure 1 of natural images a very strong prior or is it something that can be learned with a few examples? If a small number of examples is enough to discover that structure, then the conjecture in [8] about the image topology was probably incorrect. To answer that question we consider a hypothetical learning task involving images whose pixels have been permuted in a fixed but unknown way. Could we recover the 1 two-dimensional relations between pixels automatically? Could we exploit it to obtain better generalization? A related study performed in the context of ICA can be found in [1]. The basic idea of the paper is that the two-dimensional topology of pixels can be recovered by looking for a two-dimensional manifold embedding pixels (each pixel is a point in that space), such that nearby pixels have similar distributions of intensity (and possibly color) values. We explore a number of manifold techniques with this goal in mind, and explain how we have adapted these techniques in order to obtain the positive and surprising result: the two-dimensional structure of pixels can be recovered from a rather small number of training images. On images we find that the first 2 dimensions are dominant, meaning that even the knowledge that 2 dimensions are most appropriate could probably be inferred from the data. 2 Manifold Learning Techniques Used In this paper we have explored the question raised in the introduction for the particular case of images, i.e., with 2-dimensional structures, and our experiments have been performed with im- ages of size 27 ? 27 to 30 ? 30, i.e., with about a thousand pixels. It means that we have to look for the embedding of about a thousand points (the pixels) on a two-dimensional manifold. Metric Multi-Dimensional Scaling MDS is a linear embedding technique (analogous to PCA but starting from distances and yielding coordinates on the principal directions, of maximum variance). Non- parametric techniques such as Isomap [13], Local Linear Embedding (LLE) [12], or Semidefinite Embedding (SDE, also known as MVU for Maximum Variance Unfolding) [17] have computation time that scale polynomially in the number of examples n. With n around a thousand, all of these are feasible, and we experimented with MDS, Isomap, LLE, and MVU. Since we found Isomap to work best to recover the pixel topology even on small sets of images, we review the basic elements of Isomap. It applies the metric multidimensional scaling (MDS) algorithm to geodesic distances in the neighborhood graph. The neigh- borhood graph is obtained by connecting the k nearest neighbors of each point. Each arc of the graph is associated with a distance (the user-provided distance between points), and is used to compute an approximation of the geodesic dis- tance on the manifold with the length of the shortest path between two points. The metric MDS algorithm then transforms these distances into d-dimensional coordinates as follows. It first computes the dot-product (or Gram) formula, P 2n ? n1 matrix P 2M using Pthe ?double-centering? 2 2 yielding entries Mij = ? 21 (Dij ? n1 i Dij ? n j Dij + n12 i,j Dij ). The d principal eigenvectors vk?and eigenvalues ?k (k = 1, . . . , d) of M are then computed. This yields the coordinates: xik = vki ?k is the k-th embedding coordinate of point i. 3 Topology-Discovery Algorithms In order to apply a manifold learning al- gorithm, we must generally have a notion of similarity or distance between the 2 points to embed. Here each point corresponds to a pixel, and the data we have about the pixels provide an empirical distribution of intensities for all pixels. Therefore we want to compare two estimate the statistical dependency between two pixels, in order to determine if they should be ?neighbors? on the mani- fold. A simple and natural dependency statistic is the correlation between pixel intensities, and it works very well. The empirical correlation ?ij between the intensity of pixel i and pixel j is in the interval [?1, 1]. However, two pixels highly anti-correlated are much more likely to be close than pixels not corre- lated (think of edges in an image). We should thus consider the absolute value of the correlations. If we assume them to be the value of a Gaussian kernel 1 2 —?ij — = K(xi , xj ) = e? 2 kxi ?xj k , then by defining Dij = kxi ? xj k and solving the above for Dij we obtain a ?distance? formula that can be used with the manifold learning algorithms: q Dij = ? log —?ij — . (1) Note that scaling the distances in the Gaussian kernel by a variance parameter would only scale the resulting embedding, so it is unnecessary. 2 Many other measures of distance would probably work as well. However, we found the absolute correlation to be simple and easy to understand while yielding nice embeddings. 3.1 Dealing With Low-Variance Pixels A difficulty we observed in experimenting with different manifold learning algorithms on data sets such as MNIST is the influence of low-variance pixels. On MNIST digit images the border pixels may have 0 or very small variance. This makes them all want to be close to each other, which tends to fold the manifold on itself. To handle this problem we have simply ignored pixels with very low variance. When these represent a fixed background (as in MNIST images), this strategy works fine. In the experiments with MNIST we removed pixels with standard deviation less than 15% of the maximum standard deviation (maximum over all pixels). On the NORB dataset, which has varied backgrounds, this step does not remove any of the pixels (so it is unnecessary). 4 Converting Back to a Grid Image Once we have obtained an embedding for the pixels, the next thing we would like to do is to transform the data vectors back into images. For this purpose we have performed the following two steps: 1. Choosing horizontal and vertical axes (since the coordinates on the manifold can be arbitrarily rotated), and rotating the embedding coordinates accordingly, and 2. Transforming the input vector of intensity values (along with the pixel coordinates) into an ordinary discrete image on a grid. This should be done so that the resulting intensity at position (i, j) is close to the intensity values associated with input pixels whose embedding coordinates are (i, j). Such a mapping of pixels to a grid has already been done in [4], where a grid topology is defined by the connections in a graphical model, which is then trained by maximizing the approximate likelihood. However, they are not starting from a continuous embedding, but from the original data. Let pk (k = 1 . . . N ) be the embedding coordinates found by the dimensionality reduction algorithm for the k-th input variable. We select the horizontal axis as the direction of smaller spread, the vertical axis being in the orthogonal direction, and perform 3 the appropriate rotation. Once we have a coordinate system that assigns a 2- dimensional position p k to the k-th input pixel, placed at irregular locations inside a rectangular grid, we can map the input intensities x k into intensities Mi,j , so as to obtain a regular image that can be processed by standard image- processing and machine vision learning algorithms. The output image pixel intensity M i,j at coordinates (i, j) is obtained through a convex average X Mi,j = wi,j,k xk (2) k where the weights are non-negative and sum to one, and are chosen as follows. vi,j,k wi,j,k = P k vi,j,k with an exponential of the L1 distance to give less weight to farther points: vi,j,k = exp (?k(i, j) ? pk k1 ) N (i,j,k) (3) where N (i, j, k) is true if k(i, j) ? pk k1 ¡ 2 (or inferior to a larger radius to make sure that at least one input pixel k is associated with output grid position (i, j)). We used ? = 3 in the experiments, after trying only 1, 3 and 10. Large values of ? correspond to using only the nearest neighbor of (i, j) among the pk s. Smaller values smooth the intensities and make the output look better if the embedding is not perfect. Too small values result in a loss of effective resolution. 3 Algorithm 1 Pseudo-code of the topology-learning learning that recovers the 2-D structure of inputs provided in an arbitrary but fixed order. Input: X {Raw input n ? N data matrix, one row per example, with elements in fixed but arbitrary order} Input: ? = 0.15 (default value){Minimum relative standard deviation threshold, to remove too low-variance pixels} Input: k = 4?(default value){Number of neighbors used to build Isomap neighborhood graph} ? Input: L = N , W = N (default values) {Dimensions (length L, width W of output image)} Input: ? = 3 (default value) {Smoothing coefficient to recover images} Output: p {N ? 2 matrix of embedding coordinates (one per row) for each input variable} Output: w {Convolution weights to recover an image from a raw input vector} n = number of examples (rows of X) for all column P X.i do ?i ? n1 t Xti {Compute means} P ?i2 ? n1 t (Xti ? ?i )2 {Compute variances} end for Remove columns of X for which max?ji ?j ¡ ? for all column X.i do for all column X.j do (X.i ??i )0 (X.j ??j ) empirical correlation ?ij = {Compute all pair-wise empirical correla?i ?j tions} p pseudo-distances Dij = ? log —?ij — end for end for {Compute the 2-D embeddings (pk1 , pk2 ) of each input variable k through Isomap} p = Isomap(D, k, 2) {Rotate the coordinates p to try to align them to a vertical-horizontal grid (see text)} {Invert the axes if L ¡ W } {Compute the convolution weights that will map raw values to output image pixel intensities} for all grid position (i, j) in output image (i in 1 . . . L, j in 1 . . . W ) do r=1 repeat neighbors ? {k : ——pk ? (i, j)——1 ¡ r} r ?r+1 until neighbors not empty for all k in neighbors do vk ? e?——pk ?(i,j)——1 end for wi,j,. ? 0 for all k in neighbors do v {Compute convolution weights} wi,j,k = P i,j,k k vi,j,k end for end for Algorithm 2 Convolve a raw input vector into a regular grid image, using the already discovered embedding for each input variable. Input: x {Raw input N 4 -vector (in same format as a row of X above)} Input: p {N ? 2 matrix of embed- ding coordinates (one per row) for each input variable} Input: w {Convolution weights to recover an image from a raw input vector} Output: Y {L ? W output image} for all gridPposition (i, j) in output image (i in 1 . . . L, j in 1 . . . W ) do Yi,j ? k wi,j,k xk {Perform the convolution} end for 4 5 Experimental Results We performed experiments on two sets of images: MNIST digits dataset and NORB object classification dataset 1 . We used the ?jittered objects and cluttered background? image set from NORB. The MNIST images are particular in that they have a white background, whereas the NORB images have more varying backgrounds. The NORB images are originally of dimension 108 ? 108; we subsampled them by 4 ? 4 averaging into 27 ? 27 images. The experiments have been performed with k = 4 neighbors for the Isomap embedding. Smaller values of k often led to unconnected neighborhood graphs, which Isomap cannot deal with. (a) Isomap embedding (b) LLE embedding (c) MDS embedding (d) MVU embedding Figure 1: Examples of embeddings discovered by Isomap, LLE, MDS and MVU with 250 training images from NORB. Each of the original pixel is placed at the location discovered by the algorithm. Size of the circle and gray level indicate the original true location of the pixel. Manifold learning produces coordinates with an arbitrary rotation. Isomap appears most robust, and MDS the worst method, for this task. In Figure 1 we compare four different manifold learning algorithms on the NORB images: Isomap, LLE, MDS and MVU. Figure 2 explains why Isomap is giving good results, especially in comparison with MDS. One the one hand, MDS is using the pseudo-distance defined in equation 1, whose relationship with the real distance between two pixels in the original image is linear only in a small neighborhood. On the other hand, Isomap uses the geodesic distances in the neighborhood graph, whose relationship with the real distance is really close to linear. (a) (b) (c) (d) Figure 2: (a) and (c): Pseudo-distance Dij (using formula 1) vs. the true distance on the grid. (b) and (d): Geodesic distance in neighborhood graph vs. the true distance on the grid. The true distance is on the horizontal axis for all figures. (a) and (b) are for a point in the upper-left corner, (c) and (d) for a point in the center. Figure 3 shows the embeddings obtained on the NORB data using different numbers of examples. In order to quantitatively evaluate the reconstruction, we applied on each embedding the similarity transformation that minimizes the Root of the Mean Squared Error (RMSE) between the co- ordinates of each pixel on the embedding, and their coordinates on the original grid, before measuring the residual error. This minimization is justified because 5 the discovered embedding could be arbitrarily rotated, isotropically scaled, and mirrored. 100 examples are enough to get a reasonable embedding, and with 2000 or more a very good embedding is obtained: the RMSE for 2000 examples is 1.13, meaning that in expectation, each pixel is off by slightly more than one. 1 Both can be obtained from Yann Le Cun?s web site: http://yann.lecun.com/. 5 9.25 10 examples 2.43 50 examples 1.68 100 examples 1.21 1000 examples 1.13 2000 examples Figure 3: Embedding discovered by Isomap on the NORB dataset, with different numbers of training samples (top row). Second row shows the same embeddings aligned (by a similarity transformation) on the original grid, third row shows the residual error (RMSE) after the alignment. Figure 4 shows the whole process of transforming an original image (with pixels possibly permuted) into an embedded image and finally into a reconstructed image as per algorithms 1 and 2. Figure 4: Example of the process of transforming an MNIST image (top) from which pixel order is unknown (second row) into its embedding (third row) and finally reconstructed as an image after rotation and convolution (bottom). In the third row, we show the intensity associated to each original pixel by the grey level in a circle located at the pixel coordinates discovered by Isomap. We also performed experiments with acoustic spectral data to see if the time- frequency topology can be recovered. The acoustic data come from the first 100 blues pieces of a publically available genre classification dataset [14]. The FFT is computed for each frame and there are 86 frames per second. The first 30 frequency bands are kept, each covering 21.51 Hz. We used examples formed by 30-frame spectrograms, i.e., just like images of size 30 ? 30. Using the first 600,000 audio samples from each recording yielded 2600 30-frames images, on which we applied our technique. Figure 5 shows the resulting embedding when we removed the 30 coordinates of lowest standard deviation (? = .15). 6 4 Eigenvalues Ratio of consecutive eigenvalues 3.5 3 2.5 2 1.5 1 0.5 0 (a) Blues embedding 1 2 6 3 4 5 6 7 8 9 10 (b) Spectrum Figure 5: Embedding and spectrum decay for sequences of blues music. 6 Discussion Although [8] argue that learning the right permutation of pixels with a flat prior might be too difficult (either in a lifetime or through evolution), our results suggest otherwise. How do we interpret that apparent contradiction? The main element of explanation that we see is that the space of permutations of d numbers is not ? d such a large class of functions. There are approximately N = 2?d de permutations (Stirling approximation) of d numbers. Since this is a finite class of functions, its VC-dimension [15] is h = log N ? d log d ? d. Hence if we had a bounded criterion (say taking values in [0, 1]) to compare different permutations and we used n examples (i.e., n images, here), we r would expect the difference between generaliza1 2 log N/? with probability 1??. Hence, with n a tion error and test error to be bounded [15] by 2 n multiple of d log d, we would expect that one could approximately learn a good permutation. When d = 400 (the number of pixels with non-negligible variance in MNIST images), d log d ? d ? 2000. This is more than what we have found necessary to recover a ?good? representation of the images, but on the other hand there are equivalent classes within the set of permutations that give as good results as far as our objective and subjective criteria are concerned: we do not care about image symmetries, rotations, and small errors in pixel placement. What is the selection criterion that we have used to recover the image structure? Mainly we have used an additional prior which gives a preference to an order for which nearby pixels have similar distributions. How specific to natural images and how strong is that prior? This may be an application of a more general principle that could be advantageous to learning algorithms as well as to brains. When we are trying to compute useful functions from raw data, it is important to discover dependencies between the input random variables. If we are going to perform computations on subsets of variables at a time (which would seem necessary when the number of inputs is very large, to reduce the amount of connecting hardware), it would seem wiser that these computations combine variables that have dependencies with each other. That directly gives rise to the notion of local connectivity between neurons associated to nearby spatial locations, in the case of brains, the same notion that is exploited in convolutional neural networks. The fact that nearby pixels are more correlated is true at many scales in natural images. This is well known and explains why Gabor-like filters often emerge when trying to learn good filters for images, e.g., by ICA [9] or Products of Experts [3, 11]. In addition to the above arguments, there is another important consideration to keep in mind. The way in which we score permutations is not 7 the way that one would score functions in an ordinary learning experiment. Indeed, by using the distributional similarity between pairs of pixels, we get not just a scalar score but d(d?1)/2 scores. Since our ?scoring function? is much more informative, it is not surprising that it allows us to generalize from many fewer examples. 7 7 Conclusion and Future Work We proved here that, even with a small number of examples, we are able to recover almost perfectly the 2-D topology of images. This allows us to use image-specific learning algorithms without specifying any prior other than the dimensionnality of the coordinates. We also showed that this algorithm performed well on sound data, even though the topology might be less obvious in that case. However, in this paper, we only considered the simple case where we knew in advance the dimensionnality of the coordinates. One could easily apply this algorithm to data whose intrinsic dimensionality of the coordinates is unknown. In that case, one would not convert the embedding to a grid image but rather keep it and connect only the inputs associated to close coordinates (performing a k nearest neighbor for instance). It is not known if such an embedding might be useful for other types of data than the ones discussed above. Acknowledgements The authors would like to thank James Bergstra for helping with the audio data. They also want to acknowledge the support from several funding agencies: NSERC, the Canada Research Chairs, and the MITACS network.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print first paper title\n",
    "for key, value in list(titles.items())[:1]:\n",
    "    print(f\"Filename: {key}\\nTitle: {value}\\n\")\n",
    "\n",
    "# Print first paper authors\n",
    "for key, value in list(authors.items())[:1]:\n",
    "    print(f\"Filename: {key}\\nAuthors: {value}\\n\")\n",
    "\n",
    "# Print first paper abstract\n",
    "for key, value in list(abstracts.items())[:1]:\n",
    "    print(f\"Filename: {key}\\nAbstract: {value}\\n\")\n",
    "\n",
    "# Print first paper body\n",
    "for key, value in list(papers.items())[:1]:\n",
    "    print(f\"Filename: {key}\\nPaperBody: {value}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. <a id='toc8_'></a>[Preprocessing Data](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. <a id='toc8_1_'></a>[Sentence Segmentation](#toc0_)\n",
    "\n",
    "-   By iterating over each key value pair in the papers dictionary we:\n",
    "    -   Perform sentence tokenization using the `sent_tokenize` function from the `nltk` library.\n",
    "    -   We account for the case that we require capitalisation to stay intact.\n",
    "        -   Doing so by using `isupper()`\n",
    "    -   We then append each normalised word to the list of normalised sentences.\n",
    "    -   Finally, we update the value in the `papers` dictionary with the normalised sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through papers dictionary and perform sentence segmentation\n",
    "for file_name, paper in papers.items():\n",
    "    sentences = sent_tokenize(paper)\n",
    "    # Create a list to store normalized sentences\n",
    "    normalized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Split the sentence into tokens\n",
    "        words = sentence.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if i == 0 or i == len(words) - 1 or word.isupper():\n",
    "                # Keep capital tokens at the beginning, end or standalone\n",
    "                normalized_word = word\n",
    "            else:\n",
    "                # Normalize lowercase for non initial capital tokens\n",
    "                normalized_word = word.lower()\n",
    "            # Append normalized word to list\n",
    "            normalized_sentences.append(normalized_word)\n",
    "    # Update the papers dict with our normalized sentences\n",
    "    papers[file_name] = ' '.join(normalized_sentences)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also remove any numbers/digits present in the sentences as they are not required for the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary digits from papers\n",
    "papers = {key: re.sub(r'\\d+', '', value) for key, value in papers.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. <a id='toc8_2_'></a>[Tokenization](#toc0_)\n",
    "\n",
    "Using the regular expression provided for in the specification, we perform word tokenization on the sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regular expression pattern for words\n",
    "WORD_PATTERN = r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\"\n",
    "\n",
    "# Create tokenizer object\n",
    "tokenizer = RegexpTokenizer(WORD_PATTERN)\n",
    "\n",
    "# Iterate through the papers dictionary and tokenize words\n",
    "for file_name, paper in papers.items():\n",
    "    words = tokenizer.tokenize(paper)\n",
    "    # Update papers dict with tokenized words\n",
    "    papers[file_name] = words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. <a id='toc8_3_'></a>[Bigrams](#toc0_)\n",
    "\n",
    "In this step we generate bigrams from the tokens using the `nltk` library.\n",
    "\n",
    "-   Using list comprehension we remove bigrams that contain any stopwords or have the separator `\"__\"`\n",
    "-   We store the top 200 bigrams in filtered_bigrams.\n",
    "\n",
    "Finally, we retokenize the words in all papers using the multi word expression tokenizer, and update the value in the papers dictionary with the new tokenized words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords file\n",
    "with open(\"../data/input/stopwords_en.txt\") as f:\n",
    "    stopwords = set(f.read().splitlines())\n",
    "\n",
    "\n",
    "# Create a single list of all words in papers\n",
    "all_words = [word for words in papers.values() for word in words]\n",
    "\n",
    "\n",
    "# Create frequency distribution of all words\n",
    "bigram_freq = nltk.FreqDist(nltk.bigrams(all_words))\n",
    "\n",
    "\n",
    "# Remove bigrams that contain stopwords or '__'\n",
    "filtered_bigrams = [(w1, w2) for (\n",
    "    w1, w2) in bigram_freq if w1 not in stopwords and w2 not in stopwords]\n",
    "filtered_bigrams = [(w1, w2) for (\n",
    "    w1, w2) in filtered_bigrams if \"__\" not in w1 and \"__\" not in w2]\n",
    "\n",
    "\n",
    "# Store the top 200 bigrams\n",
    "top_200_bigrams = filtered_bigrams[:200]\n",
    "\n",
    "\n",
    "# Create a multi word expression tokenizer with the top 200 bigrams\n",
    "mwetokenizer = MWETokenizer(top_200_bigrams, separator='__')\n",
    "\n",
    "\n",
    "# Retokenize the words in all papers using the multi word expression tokenizer\n",
    "for file_name, words in papers.items():\n",
    "    papers[file_name] = mwetokenizer.tokenize(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. <a id='toc8_4_'></a>[Further Text Preprocessing](#toc0_)\n",
    "\n",
    "We perform the following preprocessing steps on the tokens based on the given specification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Using list comprehension we remove any stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through papers dictionary and remove stopwords\n",
    "for file_name, words in papers.items():\n",
    "    papers[file_name] = [word for word in words if word not in stopwords]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   By computing the frequency distribution of the tokens in all papers, we identify the context-dependent stopwords (that appear in 95% of the papers) and remove them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a frequency distribution of all words\n",
    "token_freq = nltk.FreqDist(\n",
    "    [word for words in papers.values() for word in words])\n",
    "\n",
    "# Create a set of context-dependent stopwords that appear in more than 95% of papers\n",
    "context_dependent_stopwords = set(\n",
    "    [token for token, freq in token_freq.items() if freq/len(papers) >= 0.95])\n",
    "\n",
    "# Iterate through papers dictionary and remove context-dependent stopwords\n",
    "for file_name, words in papers.items():\n",
    "    papers[file_name] = [\n",
    "        word for word in words if word not in context_dependent_stopwords]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We identify rare tokens (that appear in less than 3% papers) and remove them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of rare tokens that appear in less than 3% of papers\n",
    "rare_tokens = set(\n",
    "    [token for token, freq in token_freq.items() if freq/len(papers) < 0.03])\n",
    "\n",
    "# Iterate through papers dictionary and remove these rare tokens\n",
    "for file_name, words in papers.items():\n",
    "    papers[file_name] = [word for word in words if word not in rare_tokens]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Finally, we remove characters/symbols that are less than 3 characters long.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through papers dictionary and remove words with length less than 3\n",
    "for file_name, words in papers.items():\n",
    "    papers[file_name] = [word for word in words if len(word) >= 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. <a id='toc8_5_'></a>[Stemming](#toc0_)\n",
    "\n",
    "Once we have preprocessed the tokens according to the specification, we perform stemming using the PorterStemmer from the `nltk` library.\n",
    "\n",
    "Additionally, we account for cases where the original token was in uppercase or capitalized by preserving the original capitalization in the stemmed version of the token (this was required in the specification).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Iterate through papers dictionary and stem words\n",
    "for file_name, words in papers.items():\n",
    "    # Create initial list to store stemmed words\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        # Only stem words with length greater than 3\n",
    "        if len(word) > 3:\n",
    "            # Check if word is uppercase, lowercase or TitleCase (we need to preserve this)\n",
    "            if word.isupper():\n",
    "                stemmed_words.append(stemmer.stem(word).upper())\n",
    "            elif word[0].isupper():\n",
    "                stemmed_words.append(stemmer.stem(word).capitalize())\n",
    "            else:\n",
    "                stemmed_words.append(stemmer.stem(word))\n",
    "    # Update papers dict with stemmed words\n",
    "    papers[file_name] = stemmed_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. <a id='toc9_'></a>[Feature Conversion and Output](#toc0_)\n",
    "\n",
    "Iterating over all values in the `papers` dictionary we add aggregate the tokens into a list.\n",
    "\n",
    "We remove all duplicates from the list and sort the list in alphabetical order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single list of all words in papers\n",
    "all_tokens = []\n",
    "for word in papers.values():\n",
    "    all_tokens.extend(word)\n",
    "\n",
    "# Create a set of all tokens\n",
    "all_tokens = set(all_tokens)\n",
    "\n",
    "# Sort the tokens alphabetically\n",
    "all_tokens = sorted(all_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 3927\n"
     ]
    }
   ],
   "source": [
    "# Print the number of tokens\n",
    "print(f\"Number of tokens: {len(all_tokens)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We map each token in our list of tokens to the index in the list (that was alphabetically sorted).\n",
    "\n",
    "Doing so, we can create a mapping of each token to its index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict to store index of each token\n",
    "token_index = {}\n",
    "for i, token in enumerate(all_tokens):\n",
    "    token_index[token] = i\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we output the the vocabulary index file with the format given in the specification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write vocab index to file\n",
    "with open(\"../data/output/vocab.txt\", \"w\") as f:\n",
    "    for token, index in token_index.items():\n",
    "        f.write(f\"{token}:{index}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we iterate over each paper in our `papers` dictionary and create a sparse count vector for the paper using the token index dictionary.\n",
    "\n",
    "The format is kept same as the specification where we have the paper ID (filename) and sparse count vector for each paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write count vectors to file\n",
    "with open(\"../data/output/count_vectors.txt\", \"w\") as f:\n",
    "    for paper_id, paper in papers.items():\n",
    "        # Create dict to store counts of each token\n",
    "        counts = {}\n",
    "        for token in paper:\n",
    "            if token in counts:\n",
    "                counts[token] += 1\n",
    "            else:\n",
    "                counts[token] = 1\n",
    "        # Create list to store sparse counts\n",
    "        sparse_counts = []\n",
    "        for token, count in counts.items():\n",
    "            if token in token_index:\n",
    "                # Get index of token from token_index dict\n",
    "                index = token_index[token]\n",
    "                sparse_counts.append(f\"{index}:{count}\")\n",
    "        # Join sparse counts with commas\n",
    "        sparse_counts_str = \",\".join(sparse_counts)\n",
    "        f.write(f\"{paper_id},{sparse_counts_str}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. <a id='toc10_'></a>[Statistical Summary](#toc0_)\n",
    "\n",
    "We previously extracted all required data from the PDF files for the entities required for analysis.\n",
    "\n",
    "Now, we perform some statistical analysis on the data to find the 10 most frequent words in the titles, authors and abstracts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords file again (optional, we also have it in memory)\n",
    "with open(\"../data/input/stopwords_en.txt\") as f:\n",
    "    stopwords = set(f.read().splitlines())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set variables for pattern matching as per the specification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set regular expression pattern for words\n",
    "WORD_PATTERN = r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\"\n",
    "\n",
    "# Compile regular expression\n",
    "PATTERN = re.compile(WORD_PATTERN)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we perform the following steps:\n",
    "\n",
    "-   Tokenize the titles and abstracts using the regular expression provided in the specification.\n",
    "    -   It was set as a global variable in the code block above.\n",
    "    -   The authors did not require tokenization.\n",
    "-   We then identify the top 10 most frequent words in the titles, authors and abstracts.\n",
    "\n",
    "Note that for the entities we had already created dictionaries at the start of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists for titles and abstracts\n",
    "all_titles = []\n",
    "all_abstracts = []\n",
    "\n",
    "# Iterate through titles, tokenize and remove stopwords\n",
    "for paper_id, title in titles.items():\n",
    "    # Tokenize the title and remove stopwords\n",
    "    title_tokens = PATTERN.findall(title.lower())\n",
    "    title_tokens = [token for token in title_tokens if token not in stopwords]\n",
    "    all_titles.extend(title_tokens)\n",
    "\n",
    "# Count the most frequent terms in the titles and abstracts\n",
    "titles_count = Counter(all_titles)\n",
    "top10_titles = [term for term, count in sorted(\n",
    "    titles_count.items(), key=lambda x: (-x[1], x[0]))[:10]]\n",
    "\n",
    "\n",
    "# Iterate through abstracts, tokenize and remove stopwords\n",
    "for paper_id, abstract in abstracts.items():\n",
    "    # Tokenize the abstract and remove stopwords\n",
    "    abstract_tokens = PATTERN.findall(abstract.lower())\n",
    "    abstract_tokens = [\n",
    "        token for token in abstract_tokens if token not in stopwords]\n",
    "    all_abstracts.extend(abstract_tokens)\n",
    "\n",
    "# Count the most frequent terms in the titles and abstracts\n",
    "abstracts_count = Counter(all_abstracts)\n",
    "top10_abstracts = [term for term, count in sorted(\n",
    "    abstracts_count.items(), key=lambda x: (-x[1], x[0]))[:10]]\n",
    "\n",
    "\n",
    "# Using list comprehension, create a list of all authors\n",
    "authors_flat = [author for authors_list in authors.values()\n",
    "                for author in authors_list]\n",
    "\n",
    "# Remove whitespace from author names\n",
    "authors_flat = [author.strip() for author in authors_flat]\n",
    "\n",
    "# Count the most frequent authors\n",
    "authors_counter = Counter(authors_flat)\n",
    "top10_authors = [author for author, count in sorted(\n",
    "    authors_counter.items(), key=lambda x: (-x[1], x[0]))[:10]]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we output our statistical summary to a CSV file using the `csv` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write stats to file\n",
    "with open('../data/output/summary_stats.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write header\n",
    "    writer.writerow([\n",
    "        'top10_terms_in_abstracts',\n",
    "        'top10_terms_in_titles',\n",
    "        'top10_authors'])\n",
    "    # Write rows\n",
    "    writer.writerows(zip(\n",
    "        top10_abstracts,\n",
    "        top10_titles,\n",
    "        top10_authors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. <a id='toc11_'></a>[Summary](#toc0_)\n",
    "\n",
    "To summarise, we have performed the following tasks in this project.\n",
    "\n",
    "**Data Extraction:**\n",
    "\n",
    "-   Extracted URLs from the original PDF file.\n",
    "-   Downloaded PDF files using the extracted URLs.\n",
    "-   Extracted textual content from the downloaded PDF files.\n",
    "\n",
    "**Information Extraction:**\n",
    "\n",
    "-   Identified and extracted essential entities from the text, organizing them into dictionaries.\n",
    "\n",
    "**Data Preprocessing:**\n",
    "\n",
    "-   Divided the text into sentences through sentence segmentation.\n",
    "-   Segmented the sentences into individual words (tokenization).\n",
    "-   Generated bigrams from the tokenized words.\n",
    "-   Carried out additional text preprocessing in accordance with provided specifications.\n",
    "-   Applied stemming to words to reduce them to their base forms.\n",
    "\n",
    "**Statistical Analysis:**\n",
    "\n",
    "-   Conducted statistical analysis on the extracted entities.\n",
    "-   Computed the top 10 most frequent words within titles, authors, and abstracts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. <a id='toc12_'></a>[References](#toc0_)\n",
    "\n",
    "[1] [Information on using NLTK library for text preprocessing](https://www.nltk.org/book/)\n",
    "\n",
    "[2] [Using ThreadPool for parallel processing](https://www.digitalocean.com/community/tutorials/how-to-use-threadpoolexecutor-in-python-3#step-2-using-threadpoolexecutor-to-execute-a-function-in-threads\n",
    "\n",
    "[3] [Sorting data with lambda function](https://blogboard.io/blog/knowledge/python-sorted-lambda/)\n",
    "\n",
    "[4] [Compiling and testing regex patterns](https://pythex.org/)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "task2_xxxxxxx.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
